###############################################################################
# .env.sample  – copy to .env and fill with your real values
###############################################################################

###############################################################################
# Global choice: which LLM back‑end?
#   • ollama   – local Ollama server (default, zero‑internet required)
#   • watsonx  – IBM watsonx.ai hosted Granite/Llama models
###############################################################################
LLM_BACKEND=ollama

###############################################################################
# Watson x credentials  (ONLY if LLM_BACKEND=watsonx)
###############################################################################
WATSONX_PROJECT_ID=your‑watsonx‑project‑id
WATSONX_API_KEY=your‑watsonx‑api‑key
WATSONX_API_URL=https://bam-api.res.ibm.com/v2/text
# Optional: override default model
# WATSONX_MODEL_ID=meta-llama/llama-4-scout-17b-16e-instruct # ibm/granite-13b-instruct-v2


###############################################################################
# Ollama settings  (ONLY if LLM_BACKEND=ollama)
###############################################################################
# Base URL of your running Ollama daemon
OLLAMA_BASE_URL=http://localhost:11434
# Model to load (name[:tag]) – e.g. granite3.1-dense:8b
OLLAMA_MODEL_ID=granite3.1-dense:8b
# Auto‑pull the model if it’s missing? 1=yes, 0=no
OLLAMA_AUTO_PULL=1

###############################################################################
# Global LLM throttling
###############################################################################
LLM_MAX_QPS=8                # concurrent requests across all agents



###############################################################################
# Flask web‑app
###############################################################################
FLASK_SECRET_KEY=change‑me
PORT=8000
