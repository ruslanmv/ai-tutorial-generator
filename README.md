# AIâ€¯TutorialÂ GeneratorÂ ğŸš€  

*From raw PDF or web page to a beautifullyâ€‘structured, stepâ€‘byâ€‘step Markdown tutorialâ€”fully automated.*

---

## 1Â Â·Â Why this project?Â ğŸ¤”

Tech writers and educators spend hours distilling source material into easyâ€‘toâ€‘follow tutorials.  
**AIâ€¯TutorialÂ Generator** removes that friction:

* **Oneâ€‘click:** drop a PDF / paste a URL â†’ get a polished tutorial.  
* **Multiâ€‘agent architecture:** each agent focuses on a tiny task (retrieve, parse, analyse, structure, write, refine).  
* **Pluggable LLM backâ€‘ends:**  
  * **Local**â€¯â€”â€¯Ollama + Graniteâ€¯8â€¯B (works offline, CPUâ€‘only OK)  
  * **Cloud**â€¯â€”â€¯IBMÂ WatsonÂ x.ai (GraniteÂ 13â€¯B, LlamaÂ 3Â 2â€‘3â€¯B, MistralÂ Largeâ€¦)

---

## 2Â Â·Â ArchitectureÂ ğŸ§©

```mermaid
flowchart TD
    subgraph Retrieval["âœ‚ RetrievalÂ &Â Parsing"]
        A["â‘ Â Source URI / File"] --> B[SourceRetrieverAgent]
        B --> C[DocumentParserAgent]
    end
    subgraph Analysis["ğŸ” AnalysisÂ &Â Structuring"]
        C --> D[ContentAnalyzerAgent]
        D --> E[TutorialStructureAgent]
    end
    subgraph Generation["âœ GenerationÂ &Â Refinement"]
        E --> F[MarkdownGenerationAgent]
        F --> G[ReviewerRefinerAgent]
        G --> H["âœ…Â MarkdownÂ Tutorial"]
    end

    style Retrieval    fill:#eef,stroke:#6ba4ff,stroke-width:1px
    style Analysis     fill:#eafbe7,stroke:#5cb85c,stroke-width:1px
    style Generation   fill:#fff2e6,stroke:#f0ad4e,stroke-width:1px
````

*Each arrow is an async call; every box is an **Agent** (`run()` coroutine).*

---

## 3Â Â·Â Key featuresÂ âœ¨

| Agent                       | What it does                                                             |
| --------------------------- | ------------------------------------------------------------------------ |
| **SourceRetrieverAgent**    | Downloads a URL / accepts file upload, detects PDF vs HTML.              |
| **DocumentParserAgent**     | Uses **Docling** + Poppler to extract clean text blocks.                 |
| **ContentAnalyzerAgent**    | Tags each block (`title`,Â `code`,Â `step`â€¦) + 1â€‘sentence summary via LLM. |
| **TutorialStructureAgent**  | Produces JSON outline (IntroÂ â†’ StepsÂ â†’ ExamplesÂ â†’ Conclusion).           |
| **MarkdownGenerationAgent** | Expands outline into full Markdown with code blocks, tips, warnings.     |
| **ReviewerRefinerAgent**    | Final polish: grammar, style, length targets.                            |

---

## 4Â Â·Â ConfigurationÂ ğŸ—ï¸

All credentials live in `.env` (template at `.env.sample`).

```dotenv
# Choose the LLM backend:  watsonx   |   ollama
LLM_BACKEND=ollama

# WatsonÂ x (needed if LLM_BACKEND=watsonx)
WATSONX_PROJECT_ID=***
WATSONX_API_KEY=***
WATSONX_API_URL=https://us-south.ml.cloud.ibm.com
WATSONX_MODEL_ID=ibm/granite-13b-instruct-v2

# Ollama (needed if LLM_BACKEND=ollama)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL_ID=granite:8b-chat
OLLAMA_AUTO_PULL=1                 # pull model automatically if missing


```

> **Tip:** when `LLM_BACKEND=ollama` no external network calls are madeâ€”ideal for completely **offline** use.

---

## 5Â Â·Â InstallationÂ &Â quickÂ startÂ ğŸ› ï¸

```bash
git clone https://github.com/ruslanmv/ai-tutorial-generator.git
cd ai-tutorial-generator
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

# Local Ollama example
curl -fsSL https://ollama.ai/install.sh | sh
ollama serve &                      # starts daemon
ollama pull granite:8b-chat         # oneâ€‘time model download
```

---

## 6Â Â·Â UsageÂ ğŸš€

### CLI

```bash
python -m src.main input_docs/my_tutorial.pdf                        # prints Markdown
python -m src.main input_docs/another_article.html -o tutorial.md    # save to file
python -m src.main input_docs/my_tutorial.pdf --json                 # full JSON payload
```

### Flask Web UI

```bash
python app.py
# open http://localhost:8000
```

Upload a file or paste a URL, wait a few seconds, then download the tutorial.

---
![](assets/2025-05-14-14-04-44.png)

## 7Â Â·Â ExampleÂ ğŸ§‘â€ğŸ’»

```bash
python -m src.main https://www.adobe.com/support/products/enterprise/knowledgecenter/media/c4611_sample_explain.pdf \
       -o adobe-sample.md
code adobe-sample.md      # open in VSÂ Code
```

---

## 8Â Â·Â Supported WatsonÂ x model IDs Â (2025â€‘05)Â ğŸ“œ

| ID                               | Regions                         |
| -------------------------------- | ------------------------------- |
| **ibm/granite-13b-instruct-v2**  | au-syd, eu-de, jp-tok, us-south |
| ibm/granite-3-8b-instruct        | au-syd, eu-de, jp-tok, us-south |
| meta-llama/llama-3-2-3b-instruct | us-south                        |
| â€¦                                | *(see IBM catalogue)*           |

Validation logic in `src/config.py` ensures you pick a compatible ID.

---

## 9Â Â·Â DockerÂ ğŸ§©

```bash
docker build -t tutorial-gen .
docker run -p 8000:8000 --env-file .env tutorial-gen
```

*The image bundles Poppler + Ghostscript and installs the Ollama CLI.
If `LLM_BACKEND=ollama`, the container autoâ€‘starts `ollama serve`.*

---

## 10Â Â·Â TroubleshootingÂ ğŸ”

| Symptom                             | Fix                                                               |
| ----------------------------------- | ----------------------------------------------------------------- |
| `model â€¦ not recognised` (WatsonÂ x) | Update `.env` with a valid `WATSONX_MODEL_ID` from the table.     |
| `ollama pull â€¦ file does not exist` | Wrong tag. Use `granite:8b-chat`, `llama3`, `mistral-large`, etc. |
| `poppler-utils not found`           | `sudo apt install poppler-utils ghostscript` (Linux).             |
| Very large PDF slow                 | Split into chapters or raise `OLLAMA_NUM_CTX`.                    |
| GPU OOM                             | Switch to quantised model (`â€¦q4_K_M`) or CPU mode.                |

---

## 11Â Â·Â ProjectÂ layoutÂ ğŸ“‚

```
ai-tutorial-generator/
â”œâ”€â”€ app.py                 # Flask wizard
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ .env.sample
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ templates/             # wizard.html
â”œâ”€â”€ static/                # app.js + style.css
â””â”€â”€ src/
    â”œâ”€â”€ config.py          # picks Watsonx / Ollama
    â”œâ”€â”€ utils/ollama_helper.py
    â”œâ”€â”€ workflows.py       # endâ€‘toâ€‘end pipeline
    â”œâ”€â”€ main.py            # CV analysis demo
    â””â”€â”€ agents/            # 6 modular agents
```

---

## 12Â Â·Â ContributingÂ ğŸ¤

* Fork â†’Â feature branch â†’Â PR.
* Follow `flake8`, `black`, `mypy` conventions.
* Please add unit tests (pytest).

---

Made with â˜•Â and openâ€‘source tools.
If this project saves you time, give it a â­ï¸!

[beeai-framework]: https://github.com/beeai/beeai-framework
[Docling]: https://github.com/docling/docling
