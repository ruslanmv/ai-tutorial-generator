# src/agents/markdown_generation_agent.py

import os
import traceback
from typing import List

from langchain_core.documents import Document
from beeai_framework.backend.chat import ChatModel, ChatModelOutput
from beeai_framework.backend.message import SystemMessage, UserMessage, AssistantMessage

# ────────────────────────────────────────────────────────────────────────────
# Mock ChatModel for offline/testing
# ────────────────────────────────────────────────────────────────────────────
class _MockChatModel:
    def __init__(self, model_name: str):
        self.model_name = model_name
        print(f"[MockChatModel] Initialized for {model_name}")

    def _to_str(self, data) -> str:
        """Flatten nested content (str, .text, list) into a string."""
        if isinstance(data, str):
            return data
        if hasattr(data, "text"):
            return str(data.text)
        if isinstance(data, list):
            return " ".join(self._to_str(x) for x in data)
        return str(data)

    def create(self, *, messages, **_) -> ChatModelOutput:
        print("[MockChatModel] create called")
        # Find the last UserMessage content
        raw = ""
        for m in reversed(messages):
            if isinstance(m, UserMessage):
                raw = self._to_str(m.content)
                break
        # Fallback to any message with a .content if no UserMessage
        if not raw:
            for m in reversed(messages):
                if hasattr(m, "content"):
                    raw = self._to_str(m.content)
                    break

        # Simple parsing of outline and insights count
        lines = raw.splitlines()
        outline_preview = next((l for l in lines if l.startswith("Outline:")), "Outline: (none)")
        insights_count = sum(1 for l in lines if l.strip().startswith("- ("))

        # Construct mock Markdown tutorial
        mock_md = f"""# Mock Tutorial Generated by _MockChatModel

Based on: {outline_preview}

Using {insights_count} insights.

## Introduction
Mock introduction text.

## Steps
1. Step one detail.
2. Step two detail.

## Conclusion
Mock conclusion text.
"""

        assistant_msg = AssistantMessage(content=mock_md)
        return ChatModelOutput(
            model_name=self.model_name,
            messages=[assistant_msg],
            choices=[]
        )


# ────────────────────────────────────────────────────────────────────────────
# MarkdownGenerationAgent
# ────────────────────────────────────────────────────────────────────────────
class MarkdownGenerationAgent:
    """
    Takes a Markdown outline and analyzed content blocks (insights),
    then generates the full tutorial in Markdown format via an LLM.
    """

    def __init__(
        self,
        *,
        use_mocks: bool = False,
        model_name: str = "ollama:granite3.1-dense:8b",
    ):
        self.use_mocks = use_mocks
        self.model_name = (model_name or "").strip() or "ollama:granite3.1-dense:8b"

        if self.use_mocks:
            print("[MarkdownGenerationAgent] Using mock ChatModel")
            self.chat_model = _MockChatModel(self.model_name)
        else:
            try:
                print(f"[MarkdownGenerationAgent] Loading real ChatModel '{self.model_name}'")
                self.chat_model = ChatModel.from_name(self.model_name)
                print("[MarkdownGenerationAgent] Real ChatModel loaded.")
            except Exception as e:
                print(f"[MarkdownGenerationAgent][ERROR] Failed to load real ChatModel: {e}")
                print("[MarkdownGenerationAgent] Falling back to mock ChatModel")
                self.use_mocks = True
                self.chat_model = _MockChatModel(self.model_name)

        self._system_prompt = SystemMessage(content="""\
You are an expert technical writer tasked with creating a Markdown tutorial.
You will receive:
- A Markdown outline (fenced in ```markdown ... ```)
- A list of analyzed content blocks (“insights”), each prefixed with its role.

Instructions:
1. Follow the outline structure exactly.
2. Integrate and expand upon each insight in the appropriate section.
3. Use clear, concise, and well-formatted Markdown.
4. Return only the Markdown tutorial content, no additional commentary.
""")

    def run(self, outline: Document, insights: List[Document]) -> Document:
        print(f"[MarkdownGenerationAgent] Run called (mocks={self.use_mocks})")
        if not outline or not outline.page_content.strip():
            error = "# Error\n\nNo valid outline provided."
            return Document(page_content=error, metadata={"role": "tutorial_error"})

        # Build insights string
        insights_str = "\n".join(
            f"- ({doc.metadata.get('role','unknown')}) {doc.page_content.strip()}"
            for doc in insights
        )
        print(f"[MarkdownGenerationAgent] Formatted {len(insights)} insights.")

        # Build user message
        user_msg_content = (
            f"Outline:\n```markdown\n{outline.page_content.strip()}\n```\n\n"
            f"Insights:\n{insights_str}"
        )
        user_msg = UserMessage(content=user_msg_content)

        try:
            # Call chat_model.create with keyword args
            run_obj = self.chat_model.create(messages=[self._system_prompt, user_msg])

            # Unwrap any Run layers
            output = run_obj
            try:
                while hasattr(output, "result") and callable(output.result):
                    output = output.result()
            except Exception as e_unwrap:
                print(f"[MarkdownGenerationAgent][WARNING] Error unwrapping Run: {e_unwrap}")

            # Extract the text content
            if hasattr(output, "get_text_content"):
                tutorial_md = output.get_text_content().strip()
            elif hasattr(output, "messages"):
                parts = []
                for m in getattr(output, "messages", []):
                    cnt = getattr(m, "content", None)
                    if cnt is None:
                        parts.append(str(m))
                    elif isinstance(cnt, list):
                        parts.extend(str(x) for x in cnt)
                    else:
                        parts.append(str(cnt))
                tutorial_md = "\n".join(parts).strip()
            else:
                tutorial_md = str(output).strip()

            print(f"[MarkdownGenerationAgent] Generated markdown ({len(tutorial_md)} chars).")
            return Document(
                page_content=tutorial_md,
                metadata={"role": "tutorial_draft", "status": "generated"}
            )

        except Exception as e:
            print(f"[MarkdownGenerationAgent][ERROR] Generation failed: {e}")
            traceback.print_exc()
            error_md = f"# Generation Error\n\n```\n{e}\n```"
            return Document(page_content=error_md, metadata={"role": "tutorial_error"})


# ────────────────────────────────────────────────────────────────────────────
# Demo / test harness
# ────────────────────────────────────────────────────────────────────────────
if __name__ == "__main__":
    agent = MarkdownGenerationAgent(use_mocks=False, model_name="ollama:granite3.1-dense:8b")

    outline_doc = Document(
        page_content=(
            "# Tutorial Outline\n\n"
            "## Introduction\n"
            "- What this tutorial covers\n\n"
            "## Steps\n"
            "1. Step one\n"
            "2. Step two\n\n"
            "## Conclusion\n"
            "- Final thoughts\n"
        ),
        metadata={}
    )

    insights = [
        Document(page_content="Mock summary: This tutorial covers AI tutorial generation.", metadata={"role":"concept"}),
        Document(page_content="Mock summary: Step one involves setup.", metadata={"role":"step"}),
        Document(page_content="Mock summary: Step two runs the tool.", metadata={"role":"step"}),
    ]

    result = agent.run(outline_doc, insights)
    print("\n--- Generated Markdown Tutorial ---\n")
    print(result.page_content)
    print("\n--- End of Tutorial ---\n")
