# src/agents/markdown_generation_agent.py

import os
from typing import List
import traceback # Import traceback for better error logging

from langchain_core.documents import Document
# Import necessary classes from beeai_framework
from beeai_framework.backend.message import SystemMessage, UserMessage, AssistantMessage
from beeai_framework.backend.chat import ChatModel, ChatModelInput, ChatModelOutput

# --- Mock ChatModel for offline/testing ---
class _MockChatModel:
    def __init__(self, model_name: str):
        self.model_name = model_name
        print(f"[_MockChatModel] Initialized for {model_name}")

    # Ensure the mock create returns the correct type: ChatModelOutput
    def create(self, chat_input: ChatModelInput) -> ChatModelOutput:
        print("[_MockChatModel] Create called")
        # Very simple mock: echo an outline summary and count insights
        user_msg = next((m for m in chat_input.messages if isinstance(m, UserMessage)), None)
        content = user_msg.content if user_msg else ""

        # Assume content begins with "Outline:" then insights
        try:
            lines = content.splitlines()
            outline_preview = lines[0] if lines else "Outline: (empty)"
            # Be more robust parsing insights count, looking for the pattern from the run method
            insights_count = sum(1 for l in lines if l.strip().startswith("- ("))
            print(f"[_MockChatModel] Parsed outline preview: '{outline_preview}', Insights count: {insights_count}")
        except Exception as parse_err:
            print(f"[_MockChatModel][ERROR] Failed to parse input content: {parse_err}")
            outline_preview = "Outline: (parse error)"
            insights_count = 0

        # Generate the mock Markdown content
        mock_md = (
            f"# Mock Tutorial Generated by _MockChatModel\n\n"
            f"Based on: {outline_preview}\n\n"
            f"Using {insights_count} insights.\n\n"
            f"## Introduction\n"
            f"This is a mock introduction section.\n\n"
            f"## Steps\n"
            f"1. **Step 1:** Mock step detail.\n"
            f"2. **Step 2:** Another mock step detail.\n\n"
            f"## Conclusion\n"
            f"This is the mock conclusion section.\n"
        )
        print(f"[_MockChatModel] Generated mock markdown (length: {len(mock_md)})")

        # --- FIX: Return a proper ChatModelOutput object ---
        # 1. Create the mock response message using AssistantMessage
        mock_assistant_message = AssistantMessage(content=mock_md)

        # 2. Instantiate and return ChatModelOutput, matching the framework's structure
        output = ChatModelOutput(
            model_name=self.model_name,
            messages=[mock_assistant_message], # Provide the message list
            choices=[] # Keep choices empty unless the framework requires them for get_text_content
        )
        # --- END FIX ---
        return output
    # Removed the internal DummyOutput class

class MarkdownGenerationAgent:
    """
    Takes a Markdown outline and analyzed content blocks (insights),
    then generates the full tutorial in Markdown format via an LLM.
    """

    def __init__(
        self,
        use_mocks: bool = False,
        model_name: str = "ollama:granite3.1-dense:8b",
    ):
        """
        Args:
            use_mocks: if True, uses a local mock chat model.
            model_name: BeeAI ChatModel name (e.g., ollama:..., watsonx:...).
        """
        # Align mock check logic with workflow if necessary, this seems reasonable
        self.use_mocks = use_mocks or not os.environ.get("REPLICATE_API_TOKEN")
        self.model_name = model_name # Store for use

        if self.use_mocks:
            print("[MarkdownGenerationAgent] Initializing: Using mock ChatModel")
            self.chat_model = _MockChatModel(self.model_name) # Pass model name to mock
        else:
            print(f"[MarkdownGenerationAgent] Initializing: Attempting to load real ChatModel '{self.model_name}'")
            try:
                # Load the actual ChatModel using the factory from beeai_framework
                self.chat_model = ChatModel.from_name(self.model_name)
                print(f"[MarkdownGenerationAgent] Real ChatModel '{self.model_name}' loaded successfully.")
            except Exception as e:
                print(f"[MarkdownGenerationAgent][ERROR] Failed to load real ChatModel '{self.model_name}': {e}")
                print("[MarkdownGenerationAgent] FALLING BACK TO MOCKS due to load failure.")
                self.use_mocks = True # Force mocks if real one failed
                self.chat_model = _MockChatModel(self.model_name)

        # System prompt guiding the generation (Refined for clarity)
        self._system_prompt = SystemMessage(content="""
You are an expert technical writer tasked with creating a Markdown tutorial.
You will receive a Markdown outline and a list of analyzed content blocks ('insights').

Your goal is to produce a coherent, well-structured, and fully formatted Markdown tutorial.

Instructions:
1.  Strictly follow the structure and headings provided in the 'Outline' section.
2.  Integrate the information from the 'Insights' section into the relevant parts of the tutorial. Each insight is prefixed with its determined role (e.g., '(step)', '(concept)', '(code)'). Use this role to place the insight appropriately within the outline structure.
3.  Expand upon the insights, ensuring clarity, accuracy, and good writing style. Do NOT just list the insights verbatim.
4.  Format the output consistently using standard Markdown (headings, lists, bold text, code blocks, etc.).
5.  Return ONLY the complete Markdown tutorial content. Do not include any preamble, introduction, or explanation like "Here is the tutorial:" or "```markdown".
""".strip())

    def run(self, outline: Document, insights: List[Document]) -> Document:
        """
        Generates the final Markdown tutorial.

        Args:
            outline: Document containing the Markdown outline in .page_content.
            insights: List of Documents from ContentAnalyzerAgent.
                      Each has metadata['role'] and .page_content (which is the summary/analysis).

        Returns:
            Document: .page_content is the generated Markdown tutorial,
                      metadata={'role': 'tutorial_draft'} or 'tutorial_error' on failure.
                      (Using 'tutorial_draft' for successful generation before refinement)
        """
        print(f"[MarkdownGenerationAgent] Run called. Mocks enabled: {self.use_mocks}")
        if not outline or not outline.page_content or not outline.page_content.strip():
            error_md = "# Error\n\nMarkdownGenerationAgent: No valid outline provided for tutorial generation."
            print("[MarkdownGenerationAgent][ERROR] Outline is missing or empty.")
            return Document(page_content=error_md, metadata={"role": "tutorial_error"})

        # Build a single user message combining outline and insights
        # Use the summary/analyzed content from the insight document's page_content
        insights_str = "\n".join(
            # Ensure clean formatting for each insight line
            f"- ({doc.metadata.get('role','unknown').strip()}) {doc.page_content.strip()}"
            for doc in insights
        )
        print(f"[MarkdownGenerationAgent] Formatted insights string (first 300 chars): {insights_str[:300]}...")

        # Structure the user content clearly for the LLM
        user_content = (
            f"Outline:\n```markdown\n{outline.page_content.strip()}\n```\n\n"
            f"---\n" # Separator
            f"Insights:\n{insights_str}"
        )
        print(f"[MarkdownGenerationAgent] Combined user message content (first 400 chars): {user_content[:400]}...")
        user_msg = UserMessage(content=user_content)

        try:
            # Determine model identifier for logging (might be mock or real)
            model_id = getattr(self.chat_model, 'model_name', 'unknown_model')
            print(f"[MarkdownGenerationAgent] Calling chat_model.create (model: {model_id})")

            chat_input = ChatModelInput(messages=[self._system_prompt, user_msg])
            # Expect ChatModelOutput from both real and mock models now
            output: ChatModelOutput = self.chat_model.create(chat_input)

            print("[MarkdownGenerationAgent] chat_model.create call completed.")
            # get_text_content() should work correctly on the returned ChatModelOutput
            tutorial_md = output.get_text_content().strip()
            print(f"[MarkdownGenerationAgent] Received markdown content (length: {len(tutorial_md)}).")

            # Check if the LLM (or mock) returned empty content
            if not tutorial_md:
                 print("[MarkdownGenerationAgent][WARNING] Generated markdown content is empty.")
                 # Return a specific state or minimal content instead of erroring immediately
                 return Document(page_content="# Warning: Empty Tutorial Generated by Agent", metadata={"role": "tutorial_draft", "status": "empty"})

            # Return success document - use 'tutorial_draft' role
            return Document(page_content=tutorial_md, metadata={"role": "tutorial_draft", "status": "generated"})

        except Exception as e:
            # Log the full error if something goes wrong
            print(f"[MarkdownGenerationAgent][ERROR] Exception during generation: {e}")
            traceback.print_exc() # Print full traceback for detailed debugging
            error_md = f"# Generation Error\n\nMarkdownGenerationAgent encountered an error:\n\n```\n{traceback.format_exc()}\n```"
            # Return error document
            return Document(page_content=error_md, metadata={"role": "tutorial_error"})