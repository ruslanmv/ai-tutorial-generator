###############################################################################
# .env.sample  – copy to .env and fill in your own credentials / settings
###############################################################################
# Choose which LLM backend the application should use:
#   • ollama   – local Ollama server (default, no Internet required)
#   • watsonx  – IBM watsonx.ai hosted API
LLM_BACKEND=ollama

###############################################################################
# Watson x credentials  (ONLY if LLM_BACKEND=watsonx)
###############################################################################
WATSONX_PROJECT_ID=your‑watsonx‑project‑id
WATSONX_API_KEY=your‑watsonx‑api‑key
WATSONX_API_URL=https://bam-api.res.ibm.com/v2/text
# Optional: override the default model
# WATSONX_MODEL_ID=meta-llama/llama-4-scout-17b-16e-instruct

###############################################################################
# Ollama settings  (ONLY if LLM_BACKEND=ollama)
###############################################################################
# URL of your Ollama daemon (default = local host machine)
OLLAMA_BASE_URL=http://localhost:11434
# Override the model pulled with `ollama pull`
# OLLAMA_MODEL_ID=granite:8b-instruct-q4_K_M


###############################################################################
# Flask web‑app
###############################################################################
FLASK_SECRET_KEY=change‑me
PORT=8000
